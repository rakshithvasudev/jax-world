# Jax Land
Welcome! Lets explore the beautiful world of Jax environment together :)


## [Jax](https://github.com/google/jax)
Jax = Numpy + Different kinds of autograd  + H/W Acceleration + XLA/JIT
Lets not forget the functional composition.

## [Optax](https://github.com/deepmind/optax)
Gradient processing and optimization with JAX. Transformations like JAX. Has implementation of :
- Optimizers 
- Loss functions

## [Flax](https://github.com/google/jax#transformations)
Research ready deep learning framework on JAX. Modularity. Supports pytorch like API. Has:
- Neural net APIs
- Optimizers
- Works with large training setups. Accelerated, Multi-node.

## [Haiku](https://github.com/deepmind/dm-haiku)
Another JAX core neural net library. Not a framework.
- Composition
- Tensorflow like with JAX flexibility
- Supports Pytorch like subclassing
- Supports Scale - Distributed training (pmap)
- No optimizers
- Sonnet like programming model
- Ability to inspect and manipulate entires at each layer level
- save internal states whenever needed
- 





